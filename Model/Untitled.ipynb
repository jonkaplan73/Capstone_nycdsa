{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model needs to import\n",
      "model needs to import\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, './utility_files')\n",
    "import cleanData\n",
    "\n",
    "\n",
    "sys.path.insert(0, './utility_files')\n",
    "import PreProcess_LogisticReg\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "\n",
    "#Import data\n",
    "dataFinal = pd.read_csv('./data/input_data.csv')\n",
    "dataFinal['merchant_string'] = dataFinal['merchant_string'].astype(str)\n",
    "dataFinal_copy = dataFinal.copy()\n",
    "\n",
    "model = 'both' #'lr','nb', 'both'\n",
    "\n",
    "if model !=  'lr':\n",
    "    #Import nb model if they don't exist but of they are stored in memory then do not re-import them\n",
    "    try:\n",
    "        count_vec\n",
    "        multiNB \n",
    "    except:\n",
    "        print('model needs to import')\n",
    "        def try_to_load_as_pickled_object_or_None(filepath):\n",
    "            \"\"\"\n",
    "            This is a defensive way to write pickle.load, allowing for very large files on all platforms\n",
    "            \"\"\"\n",
    "            max_bytes = 2**31 - 1\n",
    "\n",
    "            input_size = os.path.getsize(filepath)\n",
    "            bytes_in = bytearray(0)\n",
    "            with open(filepath, 'rb') as f_in:\n",
    "                for _ in range(0, input_size, max_bytes):\n",
    "                    bytes_in += f_in.read(max_bytes)\n",
    "            obj = pickle.loads(bytes_in)\n",
    "\n",
    "            return obj\n",
    "\n",
    "        count_vec = try_to_load_as_pickled_object_or_None('./NB_Models_ModelCreation/final_NB_countvec.sav')\n",
    "        multiNB = try_to_load_as_pickled_object_or_None('./NB_Models_ModelCreation/final_multiNB_model.sav')\n",
    "\n",
    "    remove_string = '[]' #'^[0-9]*[0-9]$|^www$|^com$|^ave|^street$|^st$|^road$|^and$|^inc$|^at$|^drive$|^of$|^main$|^the$|^[ewns]$|^#|^[0-9]*th$|^[0-9]*rd$|^1st$|^store$|^south$'\n",
    "    split_string = \"[- ./*']\"\n",
    "    cleanData.clean(dataFinal,col='clean_nb_merchant_string',rejoin_col_strings_bi=True,lowercase=True,split_string=split_string,remove_string = remove_string,join_mcc_bi=True)\n",
    "\n",
    "    #put data into sparce matrix\n",
    "    X_test_count = count_vec.transform(dataFinal['clean_nb_merchant_string'])\n",
    "    predicted_brand_test = multiNB.predict(X_test_count)\n",
    "    probability = pd.DataFrame(multiNB.predict_proba(X_test_count))\n",
    "    probs = list(probability.max(axis=1))\n",
    "\n",
    "    #add data back to the dataframe and save .csv file with mapped_brands\n",
    "    dataFinal['Confidence_Level_NaiveBayes'] = probs\n",
    "    dataFinal['All_Predict_NaiveBayes'] = list(predicted_brand_test)\n",
    "    threshold =  .9997\n",
    "    dataFinal['Predict_NaiveBayes'] = np.where(dataFinal['Confidence_Level_NaiveBayes']>threshold,dataFinal['All_Predict_NaiveBayes'],'None')\n",
    "    dataFinal.drop('clean_nb_merchant_string',axis=1,inplace=True)\n",
    "\n",
    "\n",
    "if model !=  'nb':\n",
    "    #Import nb model if they don't exist but of they are stored in memory then do not re-import them\n",
    "    try:\n",
    "        multiLog\n",
    "    except:\n",
    "        print('model needs to import')\n",
    "        def try_to_load_as_pickled_object_or_None(filepath):\n",
    "            \"\"\"\n",
    "            This is a defensive way to write pickle.load, allowing for very large files on all platforms\n",
    "            \"\"\"\n",
    "            max_bytes = 2**31 - 1\n",
    "\n",
    "            input_size = os.path.getsize(filepath)\n",
    "            bytes_in = bytearray(0)\n",
    "            with open(filepath, 'rb') as f_in:\n",
    "                for _ in range(0, input_size, max_bytes):\n",
    "                    bytes_in += f_in.read(max_bytes)\n",
    "            obj = pickle.loads(bytes_in)\n",
    "\n",
    "            return obj\n",
    "    \n",
    "    multiLog = try_to_load_as_pickled_object_or_None('./Logistic_Regression_ModelCreation/final_ML_model.sav')\n",
    "    \n",
    "    mcc_network_cols = list(pd.read_csv('./utility_files/mcc_network_cols.csv'))    \n",
    "    mostCommonWords = list(pd.read_csv('./utility_files/most_common_words1.csv'))\n",
    "\n",
    "    PreProcess_LogisticReg.preprocess(dataFinal_copy)\n",
    "\n",
    "    #dataFinal['mapped_brand_response'] = \"\"\n",
    "    #mcc_dict = {'6011': 'atm', '6010': 'atm', '7523':'parking'}\n",
    "    #PreProcess_LogisticReg.mcc_dict_funct(dataFinal, 'mcc', 'mapped_brand_response', mcc_dict)\n",
    "\n",
    "    remove_string = '^[0-9]*[0-9]$|^www$|^com$|^ave|^street$|^road$|^and$|^inc$|^at$|^drive$|^of$|^main$|^the$|^[ewns]$|^#|^[0-9]*th$|^3rd$|^2nd$|^1st$|^store$|^st$|^rd$|^blvd$|^hwy$|^dr$'\n",
    "    split_string = '[-./ ]'\n",
    "\n",
    "    cleanData.clean(dataFinal_copy, old_col='merchant_string', col='clean_lr_merchant_string',split_string=split_string,\n",
    "        remove_string = remove_string,lowercase=False, remove_empty_strings_bi=True,\n",
    "        join_mcc_bi=False,rejoin_col_strings_bi=False)\n",
    "\n",
    "    #wordcnt_df, most_common_words, most_common_words1 = PreProcess_LogisticReg.most_common_words(dataFinal,'merchant_string_clean_lr')\n",
    "    data_dummified = dataFinal_copy.copy()\n",
    "   \n",
    "\n",
    "    # Dummify data (on most common words in merchant string cleaned)\n",
    "    PreProcess_LogisticReg.dummify_data(data_dummified, mostCommonWords)\n",
    "\n",
    "\n",
    "    # Dummify additional columns (mcc, network) and drop merchant string column\n",
    "    data_dummified=pd.get_dummies(data_dummified, prefix=['mcc', 'network'], columns=['mcc', 'network'])\n",
    "\n",
    "\n",
    "    #data_dummified.columns.contain(del_cols), axis = 1)\n",
    "    data_dummified = data_dummified.drop(['clean_lr_merchant_string','merchant_string'], axis = 1)\n",
    "\n",
    "    new_cols = list(set(mcc_network_cols) - set(data_dummified.columns))\n",
    "    for i in new_cols:\n",
    "        data_dummified[i] = 0\n",
    "\n",
    "#     print('------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "#     print(data_dummified.head())\n",
    "#     print('------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    \n",
    "#     #put data into sparce matrix\n",
    "#     print(multiLog.predict(data_dummified))\n",
    "#     predicted_brand_lr = multiLog.predict(data_dummified)\n",
    "#     dataFinal['Predictions_Logistic_Regression'] = predicted_brand_lr\n",
    "\n",
    "# if model =='both':\n",
    "#     dataFinal['Final_mapped_brand_prediction'] =  np.where(dataFinal['Confidence_Level_NaiveBayes']>threshold,dataFinal['All_Predict_NaiveBayes'],dataFinal['Predictions_Logistic_Regression'])\n",
    "\n",
    "\n",
    "# print(dataFinal.head())\n",
    "# open('./data/output_data_labeled.csv', 'w').close()\n",
    "# dataFinal.to_csv('./data/output_data_labeled.csv',index=False)\n",
    "\n",
    "# print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'sonic'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-77ad56c139e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#put data into sparce matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmultiLog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dummified\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mpredicted_brand_lr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmultiLog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dummified\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdataFinal\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Predictions_Logistic_Regression'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredicted_brand_lr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/base.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0mPredicted\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0mper\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \"\"\"\n\u001b[0;32m--> 324\u001b[0;31m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/base.py\u001b[0m in \u001b[0;36mdecision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    298\u001b[0m                                  \"yet\" % {'name': type(self).__name__})\n\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[0mn_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    446\u001b[0m         \u001b[0;31m# make sure we actually converted to numeric:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdtype_numeric\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"O\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m             \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mallow_nd\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m             raise ValueError(\"Found array with dim %d. %s expected <= 2.\"\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'sonic'"
     ]
    }
   ],
   "source": [
    "#put data into sparce matrix\n",
    "\n",
    "predicted_brand_lr = multiLog.predict(data_dummified)\n",
    "dataFinal['Predictions_Logistic_Regression'] = predicted_brand_lr\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model =='both':\n",
    "    dataFinal['Final_mapped_brand_prediction'] =  np.where(dataFinal['Confidence_Level_NaiveBayes']>threshold,dataFinal['All_Predict_NaiveBayes'],dataFinal['Predictions_Logistic_Regression'])\n",
    "\n",
    "\n",
    "print(dataFinal.head())\n",
    "open('./data/output_data_labeled.csv', 'w').close()\n",
    "dataFinal.to_csv('./data/output_data_labeled.csv',index=False)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    " \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
